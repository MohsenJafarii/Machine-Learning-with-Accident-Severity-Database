# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LM4WGiswVs3DfDQSIYvZlphHkNNIKGxB

# **Introduction of Data**
"""

# Import the pandas library to work with data frames
import pandas as pd

# Read the 'RTA Dataset.csv' file located in the user's Google Drive into a pandas DataFrame
RTA = pd.read_csv('/content/drive/MyDrive/RTA Dataset.csv')

# Display the first 5 rows of the RTA DataFrame
RTA.head()

# Display information of the RTA DataFrame columns
RTA.info()

# Calculate the percentage of missing values for each column in the RTA dataframe
percent_missing = (RTA.isnull().sum() + RTA.isna().sum()) * 100 / len(RTA)

# Create a dataframe with the column names and their corresponding percentage of missing values
missing_value_df = pd.DataFrame({'column_name': RTA.columns,
                                 'percent_missing': percent_missing})

# Display the resulting dataframe
missing_value_df

# Calculate the summary statistics for the RTA dataframe
# and round the values to 2 decimal places
print(round(RTA.describe(), 2))

# Import the pandas library
import pandas as pd

# Print the columns of the RTA dataframe as a dataframe
print(RTA.columns.to_frame())

# Print the column names of the RTA dataframe
print(RTA.columns)

# Import the pandas library
import pandas as pd

# Assign the column names of the RTA dataframe to the 'Columns' variable
Columns = RTA.columns

# Calculate the number of unique values (distinct dummies) for each column in the RTA dataframe
Distinct_dummies = RTA[Columns].nunique()

# Calculate the count of each unique value (distinct dummy) for each column in the RTA dataframe
Distinct_dummies_counts = RTA[Columns].value_counts().to_frame()

# Print the number of distinct dummies for each column
print("Distinct dummies", Distinct_dummies)

# Print the count of each distinct dummy for each column
print(Distinct_dummies_counts)

# Import the pandas library
import pandas as pd

# Create an empty dataframe to store the distinct dummies information
distinct_dummies_df = pd.DataFrame(columns=['Column', 'Distinct Dummies', 'Counts'])

# Iterate through each column in the RTA dataframe
for column in RTA.columns:
    # Calculate the number of distinct dummies in the current column
    distinct_dummies = RTA[column].nunique()

    # Calculate the count of each distinct dummy in the current column
    distinct_dummies_counts = RTA[column].value_counts(dropna=False).reset_index()
    distinct_dummies_counts.columns = ['Value', 'Counts']

    # Add the column name to the distinct dummies counts dataframe
    distinct_dummies_counts['Column'] = column

    # Concatenate the distinct dummies counts dataframe to the main dataframe
    distinct_dummies_df = pd.concat([distinct_dummies_df, distinct_dummies_counts], ignore_index=True)

# Save the distinct dummies dataframe to an Excel file
distinct_dummies_df.to_excel("output.xlsx", index=False)

# Display the distinct dummies dataframe
distinct_dummies_df

"""# **Data Preparation**"""

# Import the pandas library
import pandas as pd

# Read the RTA dataset from a CSV file located in the Google Drive
# Replace 'Unknown', 'Other', and 'na' values with NaN
RTA_filtered = pd.read_csv('/content/drive/MyDrive/RTA Dataset.csv', na_values=['Unknown', 'Other', 'na'])

# Fill any remaining NaN values with the 'Unknown' string
RTA_filtered.fillna('Unknown', inplace=True)

# Display the filtered RTA dataframe
RTA_filtered

# Display information of the RTA_filtered DataFrame columns

RTA_filtered.info()

# Identify the columns to drop
columns_to_drop = ['Casualty_severity', 'Defect_of_vehicle', 'Sex_of_driver', 'Educational_level', 'Vehicle_driver_relation', 'Owner_of_vehicle', 'Sex_of_casualty', 'Work_of_casuality']

# Drop the specified columns from the DataFrame
RTA_filtered.drop(columns=columns_to_drop, inplace=True)

# Replace 'NormalNormal' with 'Normal' in the 'Fitness_of_casuality' column
RTA_filtered['Fitness_of_casuality'] = RTA_filtered['Fitness_of_casuality'].replace('NormalNormal', 'Normal')

# Display the updated DataFrame
RTA_filtered

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt

# Load the RTA_filtered data into the RTA variable
RTA = RTA_filtered

# Extract the hour information from the 'Time' column
Hour = pd.to_datetime(RTA['Time']).dt.hour

# Extract the number of vehicles involved and number of casualties from the RTA dataframe
Number_of_vehicles_involved = RTA['Number_of_vehicles_involved']
Number_of_casualties = RTA['Number_of_casualties']

# Count the number of accidents by day of the week
day_counts = RTA['Day_of_week'].value_counts()

# Create a figure with 4 subplots
plt.figure(figsize=(18, 12))
plt.subplots_adjust(hspace=0.5, wspace=0.5)

# Subplot 1: Distribution of Accidents by Hour
plt.subplot(2, 2, 1)
plt.hist(Hour, bins=24, color='#C0C0C0', edgecolor='black')
plt.title('Distribution of Accidents by Hour', fontsize=15)
plt.xlabel('Hour')
plt.ylabel('Frequency')
plt.xticks(range(24))

# Subplot 2: Distribution of Number of Vehicles Involved
plt.subplot(2, 2, 2)
plt.hist(Number_of_vehicles_involved, color='#B0C4DE', edgecolor='black')
plt.title('Distribution of Number of Vehicles Involved', fontsize=15)
plt.xlabel('Number of Vehicles Involved')
plt.ylabel('Frequency')

# Subplot 3: Distribution of Number of Casualties
plt.subplot(2, 2, 3)
plt.hist(Number_of_casualties, color='#DDA0DD', edgecolor='black')
plt.title('Distribution of Number of Casualties', fontsize=15)
plt.xlabel('Number of Casualties')
plt.ylabel('Frequency')

# Subplot 4: Distribution of Days of the Week
plt.subplot(2, 2, 4)
weekdays = ['Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']
plt.bar(weekdays, day_counts[weekdays], color='#D2B48C', edgecolor='black')
plt.title('Distribution of Days of the Week', fontsize=15)
plt.xlabel('Day of the Week')
plt.ylabel('Frequency')

# Adjust the layout and display the plot
plt.tight_layout()
plt.show()

# Import the pandas library
import matplotlib.pyplot as plt

# Get the counts of accident severity from the RTA_filtered dataframe
data = RTA_filtered['Accident_severity'].value_counts()

# Create a bar chart to visualize the distribution of accident severity
plt.figure(figsize=(10, 6))
plt.bar(data.index, data.values)
plt.title("Distribution of Accident Severity")
plt.xlabel("Accident Severity")
plt.ylabel("Frequency")
plt.show()

# Assign the 'RTA_filtered' DataFrame to the variable 'df'
df = RTA_filtered

# Drop any duplicate rows from the 'df' DataFrame in-place
df.drop_duplicates(inplace=True)

# Calculate the difference between the number of rows in 'df' and 'RTA_filtered'
# This will give the number of duplicate rows that were removed
num_duplicates_removed = df.shape[0] - RTA_filtered.shape[0]

# Convert the 'Time' column to datetime format and extract the hour
RTA_filtered['Time'] = pd.to_datetime(RTA_filtered['Time']).dt.hour

# Assign the 'Accident_severity' column to the target variable 'y'
y = RTA_filtered['Accident_severity'].copy()

# Drop the 'Accident_severity' column from the feature matrix 'x'
x = RTA_filtered.drop(['Accident_severity'], axis=1)

# Import necessary libraries
import pandas as pd
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# Create an instance of the RandomOverSampler with default settings
oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)

# Apply the oversampling technique to the input data 'x' and labels 'y'
x_resampled, y_resampled = oversampler.fit_resample(x, y)

# Extract the 'Number_of_vehicles_involved' and 'Number_of_casualties' features from the resampled data
x_Number_of_vehicles_involved = x_resampled['Number_of_vehicles_involved']
x_Number_of_casualties = x_resampled['Number_of_casualties']

# Drop the 'Number_of_vehicles_involved' and 'Number_of_casualties' columns from the resampled data
x_resampled = x_resampled.drop(['Number_of_vehicles_involved'], axis=1)
x_resampled = x_resampled.drop(['Number_of_casualties'], axis=1)

# Create one-hot encoded columns for the resampled labels 'y_resampled'
y_dummy = pd.get_dummies(y_resampled, prefix='y')

# Create one-hot encoded columns for the resampled features 'x_resampled', excluding the first column
x_dummy = pd.get_dummies(x_resampled, prefix='x', drop_first=True)

# Concatenate the one-hot encoded features, 'Number_of_vehicles_involved', and 'Number_of_casualties' into a single DataFrame
x_dummy = pd.concat([x_dummy, x_Number_of_vehicles_involved, x_Number_of_casualties], axis=1)

# Uncomment the following lines if you want to save the resampled data to CSV files
# y_dummy = y_dummy.astype(int)
# x_dummy = x_dummy.astype(int)
# df_y_dummy = pd.DataFrame(y_dummy)
# df_x_dummy = pd.DataFrame(x_dummy)
# df_y_dummy.to_csv('data_y_dummy.csv', index=False)
# df_x_dummy.to_csv('data_x_dummy.csv', index=False)

# Import necessary libraries
from sklearn.model_selection import train_test_split
import numpy as np

# Split the resampled data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x_dummy, y_dummy, test_size=0.2, random_state=42)

# Convert the one-hot encoded labels back to their original categorical form
yr_train = np.argmax(y_train, axis=1)
yr_test = np.argmax(y_test, axis=1)

# Import necessary library
import numpy as np

# Create a DataFrame to store the evaluation results
Evaluation_Results = pd.DataFrame(np.zeros((5, 5)), columns=['Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC-ROC score'])
Evaluation_Results.index = ['Logistic Regression (LR)', 'Decision Tree Classifier (DT)', 'Random Forest Classifier (RF)',
                           'Support Vector Machine (SVM)', 'K Nearest Neighbours (KNN)']

# Install the scikitplot library
!pip install scikit-plot

# Import the necessary functions
from scikitplot.metrics import plot_roc_curve as auc_roc
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score, roc_curve, precision_score, recall_score

def AUC_ROC_plot(yr_test, pred):
    """
    Plots the ROC curve and calculates the AUC-ROC score.

    Parameters:
    yr_test (numpy.ndarray): The true labels of the test set.
    pred (numpy.ndarray): The predicted labels or probabilities of the test set.
    """
    # Calculate the AUC-ROC score for a random classifier (baseline)
    ref = [0 for _ in range(len(yr_test))]
    ref_auc = roc_auc_score(yr_test, ref)

    # Calculate the AUC-ROC score for the actual classifier
    lr_auc = roc_auc_score(yr_test, pred)

    # Plot the ROC curve
    ns_fpr, ns_tpr, _ = roc_curve(yr_test, ref)
    lr_fpr, lr_tpr, _ = roc_curve(yr_test, pred)

    plt.plot(ns_fpr, ns_tpr, linestyle='--')
    plt.plot(lr_fpr, lr_tpr, marker='.', label='AUC = {}'.format(round(roc_auc_score(yr_test, pred)*100,2)))
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.show()

def Classification_Summary(pred, pred_prob, i):
    """
    Calculates and prints the classification metrics for a given model.

    Parameters:
    pred (numpy.ndarray): The predicted labels of the test set.
    pred_prob (numpy.ndarray): The predicted probabilities of the test set.
    i (int): The index of the current model in the Evaluation_Results DataFrame.
    """
    # Calculate the classification metrics
    Evaluation_Results.iloc[i]['Accuracy'] = round(accuracy_score(yr_test, pred), 3) * 100
    Evaluation_Results.iloc[i]['Precision'] = round(precision_score(yr_test, pred, average='weighted'), 3) * 100
    Evaluation_Results.iloc[i]['Recall'] = round(recall_score(yr_test, pred, average='weighted'), 3) * 100
    Evaluation_Results.iloc[i]['F1-score'] = round(f1_score(yr_test, pred, average='weighted'), 3) * 100
    Evaluation_Results.iloc[i]['AUC-ROC score'] = round(roc_auc_score(yr_test, pred_prob, average='weighted', multi_class='ovr'), 3) * 100

    # Print the classification summary
    print('{}{}\033[1m Evaluating {} \033[0m{}{}\n'.format('<' * 3, '-' * 35, Evaluation_Results.index[i], '-' * 35, '>' * 3))
    print('Accuracy = {}%'.format(round(accuracy_score(yr_test, pred), 3) * 100))
    print('F1 Score = {}%'.format(round(f1_score(yr_test, pred, average='weighted'), 3) * 100))
    print('\n \033[1mConfusion Matrix:\033[0m\n', confusion_matrix(yr_test, pred))
    print('\n\033[1mClassification Report:\033[0m\n', classification_report(yr_test, pred))

    # Plot the ROC curve
    auc_roc(yr_test, pred_prob, curves=['each_class'])
    plt.show()

"""# **Logistic Regression**"""

# Import necessary libraries
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.linear_model import LogisticRegression
from matplotlib import pyplot

def get_models():
    """
    This function creates a dictionary of logistic regression models with different
    regularization parameters (C) and returns the dictionary.

    The models are created using the LogisticRegression class from scikit-learn.
    The multi_class parameter is set to 'multinomial' to handle multi-class
    classification, and the solver is set to 'lbfgs' to handle the optimization.

    The regularization parameter C is set to different values (0.0, 0.0001, 0.001,
    0.01, 0.1, 1.0) to create different models.
    """
    models = dict()
    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:
        key = '%.4f' % p
        if p == 0.0:
            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')
        else:
            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)
    return models

def evaluate_model(model, x_train, yr_train):
    """
    This function evaluates the performance of the given model using
    repeated stratified k-fold cross-validation.

    The RepeatedStratifiedKFold class is used to generate the train and test
    folds. The accuracy score is used as the evaluation metric.

    The function returns the scores obtained from the cross-validation.
    """
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)
    scores = cross_val_score(model, x_train, yr_train, scoring='accuracy', cv=cv, n_jobs=-1)
    return scores

# Create the dictionary of models
models = get_models()

# Evaluate each model and store the results
results, names = list(), list()
for name, model in models.items():
    scores = evaluate_model(model, x_train, yr_train)
    results.append(scores)
    names.append(name)
    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))

# Create a box plot to visualize the performance of the models
pyplot.boxplot(results, labels=names, showmeans=True)
pyplot.show()

# Import necessary libraries
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.linear_model import LogisticRegression
import numpy as np
from sklearn.metrics import f1_score
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import loguniform
from sklearn.pipeline import Pipeline
from sklearn import set_config
import joblib

# Initialize the base logistic regression model
model1 = LogisticRegression()

# Set the multi-class classification mode to 'multinomial'
multi_class='multinomial'

# Define the hyperparameter search space
space = dict()
space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']  # Options for the solver algorithm
space['penalty'] = ['l2']  # Use L2 regularization
space['C'] = loguniform(1e-5, 100)  # Log-uniform distribution for the regularization strength

# Set up the cross-validation strategy
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)

# Perform the randomized hyperparameter search
RCV = RandomizedSearchCV(model1, space, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=42)
model1 = RCV.fit(x_train, yr_train).best_estimator_  # Fit the model and get the best estimator

# Enable the 'assume_finite' option to improve handling of large datasets
set_config(assume_finite=True)

# Save the model pipeline to a file
model1_file = '/content/drive/MyDrive/model1.joblib'
model1_pipeline = Pipeline(steps=[('model1', model1)])
joblib.dump(model1_pipeline, model1_file)

# Print a heading for the output
print('\n\033[1mInterpreting the Output of Logistic Regression:\n\033[0m')

# Print the intercept value of the logistic regression model
print('intercept ', model1.intercept_[0])

# Print the class labels used in the logistic regression model
print('classes', model1.classes_)

# Display the coefficients (weights) of the logistic regression model
# in a pandas DataFrame, with the feature names as the index
display(pd.DataFrame({'coeff': model1.coef_[0]}, index=x_train.columns))

"""# **Decision Tree**"""

# Import necessary library
from math import log2

def gini_index(df, attr):
    """
    Calculates the Gini index for a given attribute in the dataframe.

    Args:
        df (pandas.DataFrame): The input dataframe.
        attr (str): The name of the attribute to calculate the Gini index for.

    Returns:
        float: The Gini index for the given attribute.
    """
    values = df[attr].unique()
    gini = 0
    for val in values:
        p = len(df[df[attr] == val]) / len(df)
        gini += p**2
    gini = 1 - gini
    return gini

def entropy(df, attr):
    """
    Calculates the entropy for a given attribute in the dataframe.

    Args:
        df (pandas.DataFrame): The input dataframe.
        attr (str): The name of the attribute to calculate the entropy for.

    Returns:
        float: The entropy for the given attribute.
    """
    values = df[attr].unique()
    entropy = 0
    for val in values:
        p = len(df[df[attr] == val]) / len(df)
        entropy += -p * log2(p)
    return entropy

# Calculate the Gini index and entropy for each attribute in the dataframe
attributes = list(x.columns)
gini_values = [gini_index(x, attr) for attr in attributes]
entropy_values = [entropy(x, attr) for attr in attributes]

# Create separate DataFrames for Gini index and entropy
gini_df = pd.DataFrame({'column_name': x.columns, 'gini_values': gini_values})
entropy_df = pd.DataFrame({'column_name': x.columns, 'entropy_values': entropy_values})

# Merge the Gini index and entropy DataFrames
merged_df = pd.concat([entropy_df, gini_df], axis=1)
merged_df.columns = ['Attribute', 'Entropy', 'Attribute', 'Gini']
merged_df

# Import necessary libraries
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from scipy.stats import randint

# Create a decision tree classifier model
model2 = DecisionTreeClassifier()

# Define the hyperparameter search space
param_dist = {
    "max_depth": [3, None],
    "max_features": randint(1, 400),
    "min_samples_leaf": randint(1, 400),
    "criterion": ["gini", "entropy"]
}

# Set up the cross-validation strategy
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)

# Perform randomized search cross-validation to optimize the hyperparameters
RCV = RandomizedSearchCV(model2, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=42)
model2 = RCV.fit(x_train, yr_train).best_estimator_

# Set the assume_finite parameter to True to avoid potential errors
set_config(assume_finite=True)

# Save the trained model to a file in Google Drive
model2_file = '/content/drive/MyDrive/model2.joblib'
model2_pipeline = Pipeline(steps=[('model2', model2)])
joblib.dump(model2_pipeline, model2_file)

# Import necessary libraries
from sklearn import tree
import pydotplus
from IPython.display import Image

# Create a decision tree classifier model
model2 = DecisionTreeClassifier()

# Define hyperparameter search space
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 400),
              "min_samples_leaf": randint(1, 400),
              "criterion": ["gini", "entropy"]}

# Set up cross-validation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)

# Perform randomized hyperparameter search
RCV = RandomizedSearchCV(model2, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=42)
model2 = RCV.fit(x_train, yr_train).best_estimator_

# Export the decision tree to a PDF file
dot_data = tree.export_graphviz(model2, out_file=None)
graph2 = pydotplus.graph_from_dot_data(dot_data)
graph2.write_pdf("Accident_Severity.pdf")

# Customize the decision tree visualization
dot_data = tree.export_graphviz(model2, out_file=None,
                     feature_names=x_dummy.columns,
                     class_names=y_dummy.columns,
                     filled=True, rounded=True,
                     special_characters=True)
graph2 = pydotplus.graph_from_dot_data(dot_data)

# Color the decision tree nodes based on class probabilities
nodes = graph2.get_node_list()
for node in nodes:
    if node.get_label():
        values = [int(ii) for ii in node.get_label().split('value = [')[1].split(']')[0].split(',')];
        color = {0: [255,255,224], 1: [255,224,255], 2: [224,255,255],}
        values = color[values.index(max(values))];
        color = '#{:02x}{:02x}{:02x}'.format(values[0], values[1], values[2]);
        node.set_fillcolor(color )

# Display the customized decision tree visualization
Image(graph2.create_png())

"""# **Random Forest**"""

# Import the necessary library for Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

# Create a Random Forest Classifier model
model3 = RandomForestClassifier()

# Define the hyperparameter search space
param_dist = {'bootstrap': [True, False],
              'max_depth': [10, 20, 50, 100, None],
              'max_features': ['auto', 'sqrt'],
              'min_samples_leaf': [1, 20, 40],
              'min_samples_split': [2, 50, 100],
              'n_estimators': [50, 100]}

# Set up cross-validation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)

# Perform randomized hyperparameter search
RCV = RandomizedSearchCV(model3, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=42)
model3 = RCV.fit(x_train, yr_train).best_estimator_

# Set the assumption that the input data is finite
set_config(assume_finite=True)

# Save the trained model to a file
model3_file = '/content/drive/MyDrive/model3.joblib'
model3_pipeline = Pipeline(steps=[('model3', model3)])
joblib.dump(model3_pipeline, model3_file)

print('\n\033[1mFeature Importance of Random Forest:\n\033[0m')
feature_importance_df = pd.DataFrame({'column_name': x_dummy.columns,'feature_importance': model3.feature_importances_})

# The feature_importance_df dataframe contains the feature importance for each column in the input data (x_dummy)
# The 'column_name' column shows the name of each feature
# The 'feature_importance' column shows the relative importance of each feature, as calculated by the Random Forest model (model3)
# The features with higher importance values are more important for the model's predictions

print(feature_importance_df)

"""# **Support Vector Machine**



"""

# Import the necessary libraries
from sklearn.svm import SVC
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn import set_config
import joblib

# Create an initial SVC model
model4 = SVC(probability=True).fit(x_train, yr_train)

# Define the hyperparameter search space for the SVC model
svm_param = {
    "C": [.01, .1, 1, 5, 10, 100],
    "gamma": [.01, .1, 1, 5, 10, 100],
    "kernel": ["rbf"],
    "random_state": [42]
}

# Set up the cross-validation strategy
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)

# Perform randomized search cross-validation to find the best hyperparameters
RCV = RandomizedSearchCV(model4, svm_param, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=42)
model4 = RCV.fit(x_train, yr_train).best_estimator_

# Set the assume_finite flag to True to avoid potential issues with the model
set_config(assume_finite=True)

# Save the final model pipeline to a file
model4_file = '/content/drive/MyDrive/model4.joblib'
model4_pipeline = Pipeline(steps=[('model4', model4)])
joblib.dump(model4_pipeline, model4_file)

"""# **K-Neareset Neighbours**"""

# Import the necessary library
from sklearn.neighbors import KNeighborsClassifier

# Create an initial KNeighborsClassifier model
model5 = KNeighborsClassifier()

# Define the hyperparameter search space for the KNeighborsClassifier model
knn_param = {
    "n_neighbors": [i for i in range(1, 10, 1)],
    "weights": ["uniform", "distance"],
    "algorithm": ["ball_tree", "kd_tree", "brute"],
    "leaf_size": [1, 10, 50],
    "p": [1, 2]
}

# Set up the cross-validation strategy
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)

# Perform randomized search cross-validation to find the best hyperparameters
RCV = RandomizedSearchCV(model5, knn_param, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=42)
model5 = RCV.fit(x_train, yr_train).best_estimator_

# Set the assume_finite flag to True to avoid potential issues with the model
set_config(assume_finite=True)

# Save the final model pipeline to a file
model5_file = '/content/drive/MyDrive/model5.joblib'
model5_pipeline = Pipeline(steps=[('model5', model5)])
joblib.dump(model5_pipeline, model5_file)

"""# **Models Evaluation**"""

# Import the necessary libraries
import joblib
import pandas as pd
from sklearn.pipeline import Pipeline

# Load the pre-trained models from the specified file paths
model1_file = '/content/drive/MyDrive/model1.joblib'
model2_file = '/content/drive/MyDrive/model2.joblib'
model3_file = '/content/drive/MyDrive/model3.joblib'
model4_file = '/content/drive/MyDrive/model4.joblib'
model5_file = '/content/drive/MyDrive/model5.joblib'
model1 = joblib.load(model1_file)
model2 = joblib.load(model2_file)
model3 = joblib.load(model3_file)
model4 = joblib.load(model4_file)
model5 = joblib.load(model5_file)

# Make predictions using the first model and get the class probabilities
y_pred1 = model1.predict(x_test)
pred_prob = model1.predict_proba(x_test)
Classification_Summary(y_pred1, pred_prob, 0)

# Make predictions using the second model and get the class probabilities
pred = model2.predict(x_test)
pred_prob = model2.predict_proba(x_test)
Classification_Summary(pred,pred_prob,1)

# Make predictions using the third model and get the class probabilities
pred = model3.predict(x_test)
pred_prob = model3.predict_proba(x_test)
Classification_Summary(pred,pred_prob,2)

# Handle the fourth model, which is a Pipeline
if isinstance(model4, Pipeline):
    print(model4.steps)
    for step_name, step in model4.steps:
        if hasattr(step, 'feature_names_in_'):
            feature_names = step.feature_names_in_
            break
    else:
        raise ValueError("Could not find feature names in the Pipeline steps.")
else:
    feature_names = model4.feature_names_

# Preprocess the test data to match the feature names expected by model4
x_test_preprocessed = x_test.copy()
x_test_preprocessed.columns = feature_names

# Make predictions using the fourth model and get the class probabilities
pred = model4.predict(x_test_preprocessed)
pred_prob = model4.predict_proba(x_test_preprocessed)
Classification_Summary(pred, pred_prob, 3)

# Make predictions using the fifth model and get the class probabilities
pred = model5.predict(x_test)
pred_prob = model5.predict_proba(x_test)
Classification_Summary(pred,pred_prob,4)

import math
import seaborn as sns

def plot_cm(yr_test, y_pred):
    # Calculate the confusion matrix
    cm = confusion_matrix(yr_test, y_pred, labels=np.unique(yr_train))

    # Calculate the percentage of each element in the confusion matrix
    cm_sum = np.sum(cm, axis=1, keepdims=True)
    cm_perc = cm / cm_sum.astype(float) * 100

    # Create a 2D array with the annotations for the heatmap
    annot = np.empty_like(cm).astype(str)
    nrows, ncols = cm.shape
    for i in range(nrows):
        for j in range(ncols):
            c = cm[i, j]
            p = cm_perc[i, j]
            if i == j:
                s = cm_sum[i]
                annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
            elif c == 0:
                annot[i, j] = ''
            else:
                annot[i, j] = '%.1f%%\n%d' % (p, c)

    # Create a DataFrame from the confusion matrix
    cm = pd.DataFrame(cm, index=np.unique(yr_test), columns=np.unique(yr_test))
    cm.columns=labels
    cm.index=labels
    cm.index.name = 'Actual'
    cm.columns.name = 'Predicted'

    # Plot the confusion matrix heatmap
    sns.heatmap(cm, annot=annot, fmt='')

def conf_mat_plot(all_models):
    labels=np.unique(yr_train)

    # Create a figure with appropriate size based on the number of models and labels
    plt.figure(figsize=[20,3.5*math.ceil(len(all_models)*len(labels)/14)])

    for i in range(len(all_models)):
        # Arrange the subplots in a grid
        if len(labels)<=4:
            plt.subplot(2,4,i+1)
        else:
            plt.subplot(math.ceil(len(all_models)/3),3,i+1)

        # Predict the labels for the test set using the current model
        if i == 3:
          pred = all_models[i].predict(x_test_preprocessed)
        else:
          pred = all_models[i].predict(x_test)

        # Plot the confusion matrix for the current model
        sns.heatmap(confusion_matrix(yr_test, pred), annot=True, cmap='Blues', fmt='.0f')
        plt.title(Evaluation_Results.index[i])

    # Adjust the layout to ensure the subplots are properly spaced
    plt.tight_layout()
    plt.show()

# Plot the confusion matrices for the given models
conf_mat_plot([model1,model2,model3,model4,model5])

# Print the title in bold
print('\033[1mMachine Learning Algorithms Comparison'.center(100))

# Plot the evaluation results as a heatmap
sns.heatmap(Evaluation_Results, annot=True, vmin=65, vmax=90, cmap='Greens', fmt='.1f')
plt.show()

# Print the evaluation results
Evaluation_Results